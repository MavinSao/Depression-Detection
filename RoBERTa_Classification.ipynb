{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MavinSao/Depression-Detection/blob/main/RoBERTa_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "9c015e75",
      "metadata": {
        "id": "9c015e75"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder,label_binarize\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn.metrics import precision_recall_curve,roc_auc_score,roc_curve"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGS-wNpkjWa1",
        "outputId": "958cdc80-50ed-41ba-d048-0df2dbc552f2"
      },
      "id": "rGS-wNpkjWa1",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List the contents of the '/content/drive' directory\n",
        "drive_path = '/content/drive/MyDrive/CODE/RoBERTa_Dep_Classification/dataset'\n",
        "drive_contents = os.listdir(drive_path)\n",
        "print(drive_contents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdNbo-qZmV_b",
        "outputId": "5896ed1b-7ca6-4e35-e6ec-7e40fad812f0"
      },
      "id": "IdNbo-qZmV_b",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train.tsv', 'test_data.tsv', 'dev_with_labels.tsv', '.ipynb_checkpoints']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "1013dd84",
      "metadata": {
        "id": "1013dd84"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(f'{drive_path}/train.tsv', sep='\\t')\n",
        "dev_df = pd.read_csv(f'{drive_path}/dev_with_labels.tsv', sep='\\t')\n",
        "test_df = pd.read_csv(f'{drive_path}/test_data.tsv', sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "79e46701",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79e46701",
        "outputId": "d5aa99f7-d3b1-4968-9c69-5451727f1e5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3245 entries, 0 to 3244\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Pid        3245 non-null   object\n",
            " 1   text data  3245 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 50.8+ KB\n"
          ]
        }
      ],
      "source": [
        "dev_df.rename(columns={'Text data': 'Text_data'}, inplace=True)\n",
        "test_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "0cccd982",
      "metadata": {
        "id": "0cccd982"
      },
      "outputs": [],
      "source": [
        "def preprocessing(df):\n",
        "    # drop unneeded columns\n",
        "    df=df.drop(['PID'],axis=1)\n",
        "\n",
        "    # Replace @,# etc by ' '\n",
        "    df['Text_data']= df['Text_data'].apply(lambda x: re.sub(r'[@#]',' ',x))\n",
        "\n",
        "    #Excluding html tags\n",
        "    df['Text_data']= df['Text_data'].apply(lambda x: re.sub(r'<[^<>]+>]',' ',x))\n",
        "\n",
        "    # Replace https links by ' '\n",
        "    df['Text_data']= df['Text_data'].apply(lambda x: re.sub(r'http://\\S+|https://\\S+',' ',x))\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "581f2786",
      "metadata": {
        "id": "581f2786"
      },
      "outputs": [],
      "source": [
        "train_df=preprocessing(train_df)\n",
        "dev_df=preprocessing(dev_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df['Label'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJpv0NUdnhS5",
        "outputId": "0356d2a1-e8db-4091-86f6-5fc1cf105334"
      },
      "id": "ZJpv0NUdnhS5",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['moderate' 'not depression' 'severe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "35622723",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "35622723",
        "outputId": "c97a1d4f-ab99-444b-e519-f841d9e5acef"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Check class imbalance in target varible\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check class imbalance in the 'Label' column\n",
        "class_counts = train_df['Label'].value_counts()\n",
        "\n",
        "# Plot the counts using a bar plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='Label', data=train_df)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "5cda0ba1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cda0ba1",
        "outputId": "bd330cc8-0140-4e0b-bede-b8df6516a269"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text_data    object\n",
              "Label        object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "train_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "e8d05617",
      "metadata": {
        "id": "e8d05617"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder,label_binarize\n",
        "\n",
        "le=LabelEncoder()\n",
        "train_df['Label']=le.fit_transform(train_df['Label'])\n",
        "dev_df['Label']=le.transform(dev_df['Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "305f2f48",
      "metadata": {
        "id": "305f2f48"
      },
      "outputs": [],
      "source": [
        "class_names=['moderate', 'not depression', 'severe']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb80d64d",
      "metadata": {
        "id": "fb80d64d"
      },
      "source": [
        "# RoBERTa model\n",
        "\n",
        "Data Preprocessing\n",
        "Let's tokenize the text to convert from raw text to numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "5d409bfd",
      "metadata": {
        "id": "5d409bfd"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import XLNetConfig, XLNetModel, XLNetTokenizer,XLNetForSequenceClassification\n",
        "from transformers import AlbertConfig, AlbertModel, AlbertTokenizer\n",
        "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer\n",
        "from transformers import AdamW, AdamWeightDecay, get_linear_schedule_with_warmup\n",
        "\n",
        "pre_trained_Robertamodel='distilroberta-base'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebd68f14",
      "metadata": {
        "id": "ebd68f14"
      },
      "source": [
        "Let's load a pre-trained RoBERT tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "d0d79f65",
      "metadata": {
        "id": "d0d79f65"
      },
      "outputs": [],
      "source": [
        "Roberta_tokenizer=RobertaTokenizer.from_pretrained(pre_trained_Robertamodel, do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "2b6f952c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b6f952c",
        "outputId": "46f9e161-8b71-479a-9a7d-88cb5ddd5aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence : I was on cloud nine, when I got a new project.\n",
            "Tokens :['I', 'Ġwas', 'Ġon', 'Ġcloud', 'Ġnine', ',', 'Ġwhen', 'ĠI', 'Ġgot', 'Ġa', 'Ġnew', 'Ġproject', '.']\n",
            "Token IDs : [100, 21, 15, 3613, 1117, 6, 77, 38, 300, 10, 92, 695, 4]\n"
          ]
        }
      ],
      "source": [
        "#Let's check a text sample to understand Roberta tokenization:\n",
        "\n",
        "text= \"I was on cloud nine, when I got a new project.\"\n",
        "\n",
        "#Convert text to tokens & token_ids\n",
        "tokens=Roberta_tokenizer.tokenize(text)\n",
        "token_ids=Roberta_tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(f'Sentence : {text}')\n",
        "print(f'Tokens :{tokens}')\n",
        "print(f'Token IDs : {token_ids}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eff3b62c",
      "metadata": {
        "id": "eff3b62c"
      },
      "source": [
        "## Add Special tokens\n",
        "XLNet expects special tokens for each sentence,\n",
        "\n",
        "- [SEP] - marker at the end of each sentence\n",
        "- [CLS] - marker at the start of each sentence.\n",
        "- [PAD] - marker for padding sentence to a specific length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "96ae84ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96ae84ff",
        "outputId": "97fb0a83-b3a4-4274-e3d7-f18846ba5e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "encoding=Roberta_tokenizer.encode_plus(\n",
        "    text,\n",
        "    max_length=32,\n",
        "    add_special_tokens=True,   # 'Add [SEP] & [CLS]'\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,  # Reurns array of 0's & 1's to distinguish padded tokens from real tokens.\n",
        "    return_token_type_ids=False,\n",
        "    return_tensors='pt'         # Returns pytorch tensors\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd0aa620",
      "metadata": {
        "id": "bd0aa620"
      },
      "source": [
        "Encoding contains input_ids & attention masks of same length 32 for each sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "952e06dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "952e06dc",
        "outputId": "7397017a-b6df-47f3-f7d8-3ff0d62721e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum length of input_ids for each sentence : 32\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   0,  100,   21,   15, 3613, 1117,    6,   77,   38,  300,   10,   92,\n",
              "         695,    4,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "# Check input_ids\n",
        "print('Maximum length of input_ids for each sentence : {}'.format(len(encoding['input_ids'][0])))\n",
        "encoding['input_ids'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bf18067",
      "metadata": {
        "id": "8bf18067"
      },
      "source": [
        "In above tensor,\n",
        "1 - Padded indexs (post -padding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "2ac5b705",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ac5b705",
        "outputId": "dd95427f-972d-40bf-bea4-9cde1b9d9e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum length of attention mask for each sentence : 32\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "#check attention mask\n",
        "print(f\"Maximum length of attention mask for each sentence : {len(encoding['attention_mask'][0])}\")\n",
        "encoding['attention_mask'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "136283f7",
      "metadata": {
        "id": "136283f7"
      },
      "source": [
        "Let's have a look at the special tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "3418d61a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3418d61a",
        "outputId": "46473732-2d13-4807-d4ff-1fcd9a195be0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " 'I',\n",
              " 'Ġwas',\n",
              " 'Ġon',\n",
              " 'Ġcloud',\n",
              " 'Ġnine',\n",
              " ',',\n",
              " 'Ġwhen',\n",
              " 'ĠI',\n",
              " 'Ġgot',\n",
              " 'Ġa',\n",
              " 'Ġnew',\n",
              " 'Ġproject',\n",
              " '.',\n",
              " '</s>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "special_tokens=Roberta_tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
        "special_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "91095045",
      "metadata": {
        "id": "91095045"
      },
      "outputs": [],
      "source": [
        "token_lens=[]\n",
        "for sentence in train_df.Text_data:\n",
        "  tokens=Roberta_tokenizer.encode(sentence,max_length=150,truncation=True)\n",
        "  token_lens.append(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "b15f71e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "b15f71e9",
        "outputId": "047f3323-0d1e-401b-c34e-781826cec643"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-62-d32ff3e56e92>:3: UserWarning: \n",
            "\n",
            "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
            "\n",
            "Please adapt your code to use either `displot` (a figure-level function with\n",
            "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "\n",
            "For a guide to updating your code to use the new functions, please see\n",
            "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
            "\n",
            "  sns.distplot(token_lens)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Token count')"
            ]
          },
          "metadata": {},
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Plot the tokens\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.distplot(token_lens)\n",
        "plt.xlim([0,150])\n",
        "plt.xlabel('Token count')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "209debc6",
      "metadata": {
        "id": "209debc6"
      },
      "source": [
        "# Create a Pytorch Dataset\n",
        "\n",
        "Pytorch dataset is of map-style i.e., it implements getitem & len protocols, & represents a map from indices/keys to datasamples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "fe6d9467",
      "metadata": {
        "id": "fe6d9467"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import optim, nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "seed=123\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "class Depression_Dataset(Dataset):\n",
        "    def __init__(self,text,label,tokenizer,max_length):\n",
        "        self.text=text\n",
        "        self.label=label\n",
        "        self.tokenizer=tokenizer\n",
        "        self.max_length=max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "        text = str(self.text[item])\n",
        "        label = self.label[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "             max_length=self.max_length,\n",
        "            add_special_tokens=True,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_tensors='pt'\n",
        "           )\n",
        "\n",
        "\n",
        "        return {\n",
        "            'text':text,\n",
        "            'input_ids':encoding['input_ids'].flatten(),\n",
        "            'attention_mask':encoding['attention_mask'].flatten(),\n",
        "            'label' : torch.tensor(label,dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48584da5",
      "metadata": {
        "id": "48584da5"
      },
      "source": [
        "Let's split the train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "edddaa58",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edddaa58",
        "outputId": "83ed5d31-f8ca-4fd8-ecdc-32985f79a53f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Print the shape of datasets...\n",
            "Training dataset : (7557, 2) \n",
            "Dev dataset : (4496, 2)\n",
            "Validation dataset : (1334, 2)\n"
          ]
        }
      ],
      "source": [
        "df_train,df_val=train_test_split(train_df,test_size=0.15 ,random_state=123)\n",
        "\n",
        "print('Print the shape of datasets...')\n",
        "print(f'Training dataset : {df_train.shape} ')\n",
        "print(f'Dev dataset : {dev_df.shape}')\n",
        "print(f'Validation dataset : {df_val.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e863cd25",
      "metadata": {
        "id": "e863cd25"
      },
      "source": [
        "Create a function for Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "f202a3fa",
      "metadata": {
        "id": "f202a3fa"
      },
      "outputs": [],
      "source": [
        "batch_size=8\n",
        "Max_length= 140\n",
        "\n",
        "def data_loader(df,tokenizer, max_length, batch):\n",
        "  ds=Depression_Dataset(\n",
        "      text= df.Text_data.to_numpy(),\n",
        "      label=df.Label.to_numpy(),\n",
        "      tokenizer=tokenizer,\n",
        "      max_length=Max_length\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "      ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=2,\n",
        "  )\n",
        "\n",
        "# Load datasets\n",
        "train_DataLoader=data_loader(df_train, Roberta_tokenizer,Max_length,batch_size)\n",
        "dev_DataLoader=data_loader(dev_df, Roberta_tokenizer,Max_length,batch_size)\n",
        "valid_DataLoader=data_loader(df_val, Roberta_tokenizer,Max_length,batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2c3a40e",
      "metadata": {
        "id": "f2c3a40e"
      },
      "source": [
        "Let's have a look at an example batch from training DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "e9752fd4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9752fd4",
        "outputId": "a899b48d-4693-4b34-fda4-47b09c71503b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Batches:   0%|          | 0/945 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   0%|          | 1/945 [00:00<07:13,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 1/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   0%|          | 2/945 [00:00<05:38,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 2/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   0%|          | 3/945 [00:01<05:02,  3.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 3/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   0%|          | 4/945 [00:01<04:48,  3.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 4/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   1%|          | 5/945 [00:01<04:33,  3.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 5/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   1%|          | 6/945 [00:01<04:26,  3.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 6/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   1%|          | 7/945 [00:02<04:12,  3.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 7/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   1%|          | 8/945 [00:02<04:03,  3.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 8/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   1%|          | 9/945 [00:02<03:56,  3.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 9/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   1%|          | 10/945 [00:02<03:51,  4.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 10/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   1%|          | 11/945 [00:02<03:34,  4.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 11/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   1%|▏         | 12/945 [00:03<03:13,  4.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 12/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   1%|▏         | 13/945 [00:03<02:58,  5.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 13/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   1%|▏         | 14/945 [00:03<02:49,  5.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 14/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   2%|▏         | 15/945 [00:03<02:43,  5.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 15/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   2%|▏         | 16/945 [00:03<02:43,  5.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 16/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   2%|▏         | 17/945 [00:03<02:36,  5.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 17/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   2%|▏         | 18/945 [00:04<02:35,  5.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 18/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   2%|▏         | 19/945 [00:04<02:31,  6.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 19/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   2%|▏         | 20/945 [00:04<02:33,  6.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 20/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   2%|▏         | 21/945 [00:04<02:29,  6.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 21/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   2%|▏         | 22/945 [00:04<02:29,  6.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 22/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   2%|▏         | 23/945 [00:04<02:32,  6.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 23/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   3%|▎         | 24/945 [00:05<02:33,  5.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 24/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   3%|▎         | 25/945 [00:05<02:28,  6.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 25/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   3%|▎         | 26/945 [00:05<02:35,  5.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 26/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   3%|▎         | 27/945 [00:05<02:32,  6.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 27/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   3%|▎         | 28/945 [00:05<02:32,  6.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 28/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   3%|▎         | 29/945 [00:05<02:33,  5.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 29/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   3%|▎         | 30/945 [00:06<02:32,  6.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 30/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   3%|▎         | 31/945 [00:06<02:30,  6.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 31/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   3%|▎         | 32/945 [00:06<02:27,  6.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 32/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   3%|▎         | 33/945 [00:06<02:29,  6.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 33/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   4%|▎         | 34/945 [00:06<02:30,  6.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 34/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   4%|▎         | 35/945 [00:06<02:31,  6.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 35/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   4%|▍         | 36/945 [00:07<02:30,  6.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 36/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   4%|▍         | 37/945 [00:07<02:31,  5.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 37/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   4%|▍         | 38/945 [00:07<02:27,  6.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 38/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   4%|▍         | 39/945 [00:07<02:27,  6.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 39/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   4%|▍         | 40/945 [00:07<02:27,  6.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 40/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   4%|▍         | 41/945 [00:07<02:30,  6.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 41/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   4%|▍         | 42/945 [00:08<02:28,  6.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 42/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   5%|▍         | 43/945 [00:08<02:26,  6.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 43/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   5%|▍         | 44/945 [00:08<02:23,  6.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 44/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   5%|▍         | 45/945 [00:08<02:23,  6.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 45/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   5%|▍         | 46/945 [00:08<02:25,  6.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 46/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   5%|▍         | 47/945 [00:08<02:27,  6.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 47/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   5%|▌         | 48/945 [00:09<02:27,  6.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 48/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   5%|▌         | 49/945 [00:09<02:23,  6.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 49/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   5%|▌         | 50/945 [00:09<02:26,  6.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 50/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   5%|▌         | 51/945 [00:09<02:23,  6.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 51/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   6%|▌         | 52/945 [00:09<02:22,  6.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 52/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   6%|▌         | 53/945 [00:09<02:23,  6.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 53/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   6%|▌         | 54/945 [00:10<02:27,  6.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 54/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   6%|▌         | 55/945 [00:10<02:28,  5.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 55/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   6%|▌         | 56/945 [00:10<02:30,  5.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 56/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   6%|▌         | 57/945 [00:10<02:28,  6.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 57/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   6%|▌         | 58/945 [00:10<02:27,  6.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 58/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   6%|▌         | 59/945 [00:10<02:28,  5.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 59/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   6%|▋         | 60/945 [00:11<02:34,  5.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 60/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   6%|▋         | 61/945 [00:11<02:33,  5.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 61/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   7%|▋         | 62/945 [00:11<02:27,  5.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 62/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   7%|▋         | 63/945 [00:11<02:25,  6.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 63/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   7%|▋         | 64/945 [00:11<02:23,  6.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 64/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   7%|▋         | 65/945 [00:11<02:23,  6.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 65/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   7%|▋         | 66/945 [00:12<02:25,  6.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 66/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   7%|▋         | 67/945 [00:12<02:27,  5.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 67/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   7%|▋         | 68/945 [00:12<02:24,  6.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 68/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   7%|▋         | 69/945 [00:12<02:27,  5.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 69/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   7%|▋         | 70/945 [00:12<02:26,  5.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 70/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   8%|▊         | 71/945 [00:12<02:28,  5.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 71/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   8%|▊         | 72/945 [00:13<02:45,  5.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 72/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   8%|▊         | 73/945 [00:13<03:04,  4.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 73/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   8%|▊         | 74/945 [00:13<03:21,  4.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 74/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   8%|▊         | 75/945 [00:13<03:31,  4.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 75/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   8%|▊         | 76/945 [00:14<03:39,  3.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 76/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   8%|▊         | 77/945 [00:14<03:44,  3.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 77/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   8%|▊         | 78/945 [00:14<03:48,  3.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 78/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   8%|▊         | 79/945 [00:14<03:46,  3.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 79/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   8%|▊         | 80/945 [00:15<03:41,  3.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 80/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   9%|▊         | 81/945 [00:15<03:36,  3.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 81/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   9%|▊         | 82/945 [00:15<03:33,  4.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 82/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   9%|▉         | 83/945 [00:15<03:37,  3.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 83/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   9%|▉         | 84/945 [00:16<03:43,  3.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 84/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   9%|▉         | 85/945 [00:16<03:49,  3.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 85/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   9%|▉         | 86/945 [00:16<03:42,  3.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 86/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   9%|▉         | 87/945 [00:17<03:47,  3.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 87/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   9%|▉         | 88/945 [00:17<03:53,  3.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 88/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:   9%|▉         | 89/945 [00:17<03:49,  3.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 89/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  10%|▉         | 90/945 [00:17<03:54,  3.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 90/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  10%|▉         | 91/945 [00:18<03:50,  3.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 91/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  10%|▉         | 92/945 [00:18<03:48,  3.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 92/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  10%|▉         | 93/945 [00:18<03:54,  3.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 93/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  10%|▉         | 94/945 [00:18<03:46,  3.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 94/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  10%|█         | 95/945 [00:19<03:42,  3.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 95/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  10%|█         | 96/945 [00:19<03:42,  3.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 96/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  10%|█         | 97/945 [00:19<03:27,  4.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 97/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  10%|█         | 98/945 [00:19<03:06,  4.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 98/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  10%|█         | 99/945 [00:19<02:53,  4.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 99/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  11%|█         | 100/945 [00:20<02:48,  5.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 100/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  11%|█         | 101/945 [00:20<02:44,  5.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 101/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  11%|█         | 102/945 [00:20<02:36,  5.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 102/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  11%|█         | 103/945 [00:20<02:31,  5.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 103/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  11%|█         | 104/945 [00:20<02:29,  5.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 104/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  11%|█         | 105/945 [00:21<02:28,  5.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 105/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  11%|█         | 106/945 [00:21<02:26,  5.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 106/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  11%|█▏        | 107/945 [00:21<02:29,  5.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 107/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  11%|█▏        | 108/945 [00:21<02:26,  5.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 108/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  12%|█▏        | 109/945 [00:21<02:24,  5.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 109/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  12%|█▏        | 110/945 [00:21<02:22,  5.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 110/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  12%|█▏        | 111/945 [00:22<02:39,  5.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 111/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  12%|█▏        | 112/945 [00:22<02:37,  5.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 112/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  12%|█▏        | 113/945 [00:22<02:33,  5.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 113/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  12%|█▏        | 114/945 [00:22<02:28,  5.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 114/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  12%|█▏        | 115/945 [00:22<02:27,  5.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 115/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  12%|█▏        | 116/945 [00:23<02:24,  5.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 116/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  12%|█▏        | 117/945 [00:23<02:21,  5.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 117/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  12%|█▏        | 118/945 [00:23<02:23,  5.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 118/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  13%|█▎        | 119/945 [00:23<02:25,  5.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 119/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  13%|█▎        | 120/945 [00:23<02:27,  5.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 120/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  13%|█▎        | 121/945 [00:23<02:27,  5.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 121/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  13%|█▎        | 122/945 [00:24<02:23,  5.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 122/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  13%|█▎        | 123/945 [00:24<02:20,  5.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 123/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  13%|█▎        | 124/945 [00:24<02:20,  5.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 124/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  13%|█▎        | 125/945 [00:24<02:26,  5.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 125/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  13%|█▎        | 126/945 [00:24<02:25,  5.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 126/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  13%|█▎        | 127/945 [00:24<02:22,  5.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 127/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  14%|█▎        | 128/945 [00:25<02:20,  5.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 128/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  14%|█▎        | 129/945 [00:25<02:16,  5.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 129/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  14%|█▍        | 130/945 [00:25<02:16,  5.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 130/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  14%|█▍        | 131/945 [00:25<02:17,  5.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 131/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  14%|█▍        | 132/945 [00:25<02:16,  5.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 132/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  14%|█▍        | 133/945 [00:25<02:17,  5.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 133/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  14%|█▍        | 134/945 [00:26<02:21,  5.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 134/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  14%|█▍        | 135/945 [00:26<02:23,  5.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 135/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  14%|█▍        | 136/945 [00:26<02:24,  5.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 136/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  14%|█▍        | 137/945 [00:26<02:22,  5.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 137/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  15%|█▍        | 138/945 [00:26<02:21,  5.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 138/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  15%|█▍        | 139/945 [00:27<02:21,  5.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 139/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  15%|█▍        | 140/945 [00:27<02:19,  5.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 140/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  15%|█▍        | 141/945 [00:27<02:24,  5.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 141/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  15%|█▌        | 142/945 [00:27<02:22,  5.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 142/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  15%|█▌        | 143/945 [00:27<02:23,  5.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 143/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  15%|█▌        | 144/945 [00:27<02:20,  5.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 144/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  15%|█▌        | 145/945 [00:28<02:20,  5.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 145/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  15%|█▌        | 146/945 [00:28<02:07,  6.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 146/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  16%|█▌        | 147/945 [00:28<02:19,  5.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 147/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  16%|█▌        | 148/945 [00:28<02:15,  5.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 148/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  16%|█▌        | 149/945 [00:28<02:14,  5.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 149/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  16%|█▌        | 150/945 [00:28<02:11,  6.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 150/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  16%|█▌        | 151/945 [00:29<02:11,  6.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 151/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  16%|█▌        | 152/945 [00:29<02:12,  6.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 152/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  16%|█▌        | 153/945 [00:29<02:10,  6.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 153/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  16%|█▋        | 154/945 [00:29<02:10,  6.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 154/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  16%|█▋        | 155/945 [00:29<02:25,  5.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 155/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  17%|█▋        | 156/945 [00:30<02:42,  4.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 156/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  17%|█▋        | 157/945 [00:30<02:55,  4.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 157/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  17%|█▋        | 158/945 [00:30<03:10,  4.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 158/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  17%|█▋        | 159/945 [00:30<03:16,  4.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 159/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  17%|█▋        | 160/945 [00:31<03:19,  3.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 160/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  17%|█▋        | 161/945 [00:31<03:23,  3.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 161/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  17%|█▋        | 162/945 [00:31<03:20,  3.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 162/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  17%|█▋        | 163/945 [00:31<03:26,  3.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 163/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  17%|█▋        | 164/945 [00:32<03:32,  3.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 164/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  17%|█▋        | 165/945 [00:32<03:29,  3.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 165/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  18%|█▊        | 166/945 [00:32<03:27,  3.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 166/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  18%|█▊        | 167/945 [00:32<03:25,  3.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 167/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  18%|█▊        | 168/945 [00:33<03:41,  3.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 168/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  18%|█▊        | 169/945 [00:33<03:40,  3.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 169/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  18%|█▊        | 170/945 [00:33<03:43,  3.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 170/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  18%|█▊        | 171/945 [00:34<03:35,  3.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 171/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  18%|█▊        | 172/945 [00:34<03:32,  3.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 172/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  18%|█▊        | 173/945 [00:34<03:30,  3.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 173/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  18%|█▊        | 174/945 [00:34<03:29,  3.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 174/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  19%|█▊        | 175/945 [00:35<03:28,  3.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 175/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  19%|█▊        | 176/945 [00:35<03:34,  3.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 176/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  19%|█▊        | 177/945 [00:35<03:25,  3.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 177/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  19%|█▉        | 178/945 [00:36<03:20,  3.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 178/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  19%|█▉        | 179/945 [00:36<03:22,  3.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 179/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  19%|█▉        | 180/945 [00:36<03:17,  3.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 180/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  19%|█▉        | 181/945 [00:36<03:01,  4.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 181/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  19%|█▉        | 182/945 [00:36<02:49,  4.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 182/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  19%|█▉        | 183/945 [00:37<02:40,  4.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 183/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  19%|█▉        | 184/945 [00:37<02:33,  4.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 184/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  20%|█▉        | 185/945 [00:37<02:26,  5.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 185/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  20%|█▉        | 186/945 [00:37<02:19,  5.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 186/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  20%|█▉        | 187/945 [00:37<02:17,  5.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 187/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  20%|█▉        | 188/945 [00:37<02:16,  5.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 188/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  20%|██        | 189/945 [00:38<02:18,  5.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 189/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  20%|██        | 190/945 [00:38<02:17,  5.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 190/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  20%|██        | 191/945 [00:38<02:13,  5.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 191/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  20%|██        | 192/945 [00:38<02:11,  5.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 192/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  20%|██        | 193/945 [00:38<02:11,  5.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 193/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  21%|██        | 194/945 [00:39<02:11,  5.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 194/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  21%|██        | 195/945 [00:39<02:11,  5.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 195/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  21%|██        | 196/945 [00:39<02:12,  5.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 196/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  21%|██        | 197/945 [00:39<02:09,  5.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 197/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  21%|██        | 198/945 [00:39<02:09,  5.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 198/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  21%|██        | 199/945 [00:39<02:07,  5.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 199/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  21%|██        | 200/945 [00:40<02:07,  5.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 200/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  21%|██▏       | 201/945 [00:40<02:12,  5.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 201/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  21%|██▏       | 202/945 [00:40<02:15,  5.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 202/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  21%|██▏       | 203/945 [00:40<02:16,  5.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 203/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  22%|██▏       | 204/945 [00:40<02:17,  5.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 204/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  22%|██▏       | 205/945 [00:40<02:14,  5.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 205/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  22%|██▏       | 206/945 [00:41<02:16,  5.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 206/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  22%|██▏       | 207/945 [00:41<02:13,  5.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 207/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  22%|██▏       | 208/945 [00:41<02:09,  5.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 208/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  22%|██▏       | 209/945 [00:41<02:06,  5.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 209/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  22%|██▏       | 210/945 [00:41<02:04,  5.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 210/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  22%|██▏       | 211/945 [00:42<02:05,  5.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 211/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  22%|██▏       | 212/945 [00:42<02:06,  5.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 212/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  23%|██▎       | 213/945 [00:42<02:03,  5.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 213/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  23%|██▎       | 214/945 [00:42<02:03,  5.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 214/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  23%|██▎       | 215/945 [00:42<02:03,  5.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 215/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  23%|██▎       | 216/945 [00:42<02:01,  6.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 216/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  23%|██▎       | 217/945 [00:43<02:02,  5.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 217/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  23%|██▎       | 218/945 [00:43<02:08,  5.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 218/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  23%|██▎       | 219/945 [00:43<02:05,  5.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 219/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  23%|██▎       | 220/945 [00:43<02:06,  5.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 220/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  23%|██▎       | 221/945 [00:43<02:05,  5.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 221/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  23%|██▎       | 222/945 [00:43<02:06,  5.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 222/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  24%|██▎       | 223/945 [00:44<02:07,  5.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 223/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  24%|██▎       | 224/945 [00:44<02:09,  5.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 224/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  24%|██▍       | 225/945 [00:44<02:07,  5.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 225/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  24%|██▍       | 226/945 [00:44<02:03,  5.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 226/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  24%|██▍       | 227/945 [00:44<02:02,  5.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 227/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  24%|██▍       | 228/945 [00:44<02:03,  5.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 228/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  24%|██▍       | 229/945 [00:45<02:01,  5.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 229/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  24%|██▍       | 230/945 [00:45<02:05,  5.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 230/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  24%|██▍       | 231/945 [00:45<02:03,  5.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 231/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  25%|██▍       | 232/945 [00:45<02:01,  5.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 232/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  25%|██▍       | 233/945 [00:45<02:03,  5.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 233/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  25%|██▍       | 234/945 [00:45<02:06,  5.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 234/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  25%|██▍       | 235/945 [00:46<02:04,  5.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 235/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  25%|██▍       | 236/945 [00:46<02:07,  5.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 236/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  25%|██▌       | 237/945 [00:46<02:10,  5.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 237/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  25%|██▌       | 238/945 [00:46<02:27,  4.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 238/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  25%|██▌       | 239/945 [00:47<02:43,  4.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 239/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  25%|██▌       | 240/945 [00:47<02:56,  3.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 240/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  26%|██▌       | 241/945 [00:47<02:55,  4.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 241/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  26%|██▌       | 242/945 [00:47<03:03,  3.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 242/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  26%|██▌       | 243/945 [00:48<03:04,  3.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 243/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  26%|██▌       | 244/945 [00:48<03:10,  3.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 244/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  26%|██▌       | 245/945 [00:48<03:07,  3.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 245/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  26%|██▌       | 246/945 [00:49<03:11,  3.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 246/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  26%|██▌       | 247/945 [00:49<03:16,  3.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 247/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  26%|██▌       | 248/945 [00:49<03:21,  3.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 248/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  26%|██▋       | 249/945 [00:49<03:20,  3.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 249/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  26%|██▋       | 250/945 [00:50<03:14,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 250/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  27%|██▋       | 251/945 [00:50<03:05,  3.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 251/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  27%|██▋       | 252/945 [00:50<03:09,  3.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 252/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  27%|██▋       | 253/945 [00:50<03:10,  3.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 253/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  27%|██▋       | 254/945 [00:51<03:14,  3.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 254/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  27%|██▋       | 255/945 [00:51<03:16,  3.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 255/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  27%|██▋       | 256/945 [00:51<03:16,  3.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 256/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  27%|██▋       | 257/945 [00:52<03:13,  3.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 257/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  27%|██▋       | 258/945 [00:52<03:10,  3.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 258/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  27%|██▋       | 259/945 [00:52<03:10,  3.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 259/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  28%|██▊       | 260/945 [00:52<02:58,  3.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 260/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  28%|██▊       | 261/945 [00:53<02:56,  3.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 261/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  28%|██▊       | 262/945 [00:53<02:57,  3.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 262/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  28%|██▊       | 263/945 [00:53<02:53,  3.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 263/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  28%|██▊       | 264/945 [00:53<02:41,  4.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 264/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  28%|██▊       | 265/945 [00:54<02:31,  4.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 265/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  28%|██▊       | 266/945 [00:54<02:20,  4.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 266/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  28%|██▊       | 267/945 [00:54<02:13,  5.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 267/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  28%|██▊       | 268/945 [00:54<02:08,  5.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 268/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  28%|██▊       | 269/945 [00:54<02:07,  5.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 269/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  29%|██▊       | 270/945 [00:54<02:04,  5.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 270/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  29%|██▊       | 271/945 [00:55<02:02,  5.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 271/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  29%|██▉       | 272/945 [00:55<01:59,  5.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 272/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  29%|██▉       | 273/945 [00:55<02:01,  5.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 273/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  29%|██▉       | 274/945 [00:55<01:59,  5.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 274/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  29%|██▉       | 275/945 [00:55<01:59,  5.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 275/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  29%|██▉       | 276/945 [00:55<02:00,  5.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 276/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  29%|██▉       | 277/945 [00:56<01:58,  5.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 277/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  29%|██▉       | 278/945 [00:56<02:00,  5.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 278/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  30%|██▉       | 279/945 [00:56<01:59,  5.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 279/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  30%|██▉       | 280/945 [00:56<01:57,  5.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 280/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  30%|██▉       | 281/945 [00:56<01:57,  5.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 281/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  30%|██▉       | 282/945 [00:57<02:00,  5.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 282/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  30%|██▉       | 283/945 [00:57<01:58,  5.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 283/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  30%|███       | 284/945 [00:57<02:00,  5.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 284/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  30%|███       | 285/945 [00:57<01:57,  5.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 285/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  30%|███       | 286/945 [00:57<01:58,  5.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 286/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  30%|███       | 287/945 [00:57<01:58,  5.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 287/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  30%|███       | 288/945 [00:58<01:57,  5.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 288/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  31%|███       | 289/945 [00:58<01:56,  5.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 289/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  31%|███       | 290/945 [00:58<01:56,  5.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 290/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  31%|███       | 291/945 [00:58<01:56,  5.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 291/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  31%|███       | 292/945 [00:58<01:57,  5.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 292/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  31%|███       | 293/945 [00:59<01:59,  5.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 293/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  31%|███       | 294/945 [00:59<01:58,  5.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 294/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  31%|███       | 295/945 [00:59<02:01,  5.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 295/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  31%|███▏      | 296/945 [00:59<01:57,  5.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 296/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  31%|███▏      | 297/945 [00:59<01:58,  5.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 297/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  32%|███▏      | 298/945 [00:59<01:58,  5.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 298/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  32%|███▏      | 299/945 [01:00<01:57,  5.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 299/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  32%|███▏      | 300/945 [01:00<01:58,  5.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 300/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  32%|███▏      | 301/945 [01:00<01:58,  5.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 301/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  32%|███▏      | 302/945 [01:00<01:58,  5.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 302/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  32%|███▏      | 303/945 [01:00<01:59,  5.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 303/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  32%|███▏      | 304/945 [01:01<01:59,  5.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 304/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  32%|███▏      | 305/945 [01:01<01:58,  5.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 305/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  32%|███▏      | 306/945 [01:01<02:01,  5.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 306/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  32%|███▏      | 307/945 [01:01<02:01,  5.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 307/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  33%|███▎      | 308/945 [01:01<02:01,  5.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 308/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  33%|███▎      | 309/945 [01:02<02:09,  4.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 309/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  33%|███▎      | 310/945 [01:02<02:18,  4.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 310/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  33%|███▎      | 311/945 [01:02<02:26,  4.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 311/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  33%|███▎      | 312/945 [01:02<02:26,  4.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 312/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  33%|███▎      | 313/945 [01:03<02:28,  4.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 313/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  33%|███▎      | 314/945 [01:03<02:25,  4.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 314/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  33%|███▎      | 315/945 [01:03<02:27,  4.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 315/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  33%|███▎      | 316/945 [01:03<02:31,  4.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 316/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  34%|███▎      | 317/945 [01:04<02:41,  3.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 317/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  34%|███▎      | 318/945 [01:04<02:50,  3.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 318/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  34%|███▍      | 319/945 [01:04<03:04,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 319/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  34%|███▍      | 320/945 [01:05<03:05,  3.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 320/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  34%|███▍      | 321/945 [01:05<03:09,  3.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 321/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  34%|███▍      | 322/945 [01:05<03:10,  3.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 322/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  34%|███▍      | 323/945 [01:06<03:17,  3.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 323/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  34%|███▍      | 324/945 [01:06<03:07,  3.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 324/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  34%|███▍      | 325/945 [01:06<02:57,  3.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 325/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  34%|███▍      | 326/945 [01:06<03:05,  3.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 326/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  35%|███▍      | 327/945 [01:07<02:57,  3.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 327/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  35%|███▍      | 328/945 [01:07<02:50,  3.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 328/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  35%|███▍      | 329/945 [01:07<02:43,  3.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 329/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  35%|███▍      | 330/945 [01:07<02:44,  3.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 330/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  35%|███▌      | 331/945 [01:08<02:51,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 331/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  35%|███▌      | 332/945 [01:08<02:47,  3.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 332/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  35%|███▌      | 333/945 [01:08<02:51,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 333/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  35%|███▌      | 334/945 [01:09<02:52,  3.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 334/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  35%|███▌      | 335/945 [01:09<02:50,  3.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 335/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  36%|███▌      | 336/945 [01:09<02:53,  3.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 336/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  36%|███▌      | 337/945 [01:09<02:57,  3.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 337/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  36%|███▌      | 338/945 [01:10<02:51,  3.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 338/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  36%|███▌      | 339/945 [01:10<02:49,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 339/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  36%|███▌      | 340/945 [01:10<02:43,  3.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 340/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  36%|███▌      | 341/945 [01:10<02:40,  3.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 341/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  36%|███▌      | 342/945 [01:11<02:37,  3.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 342/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  36%|███▋      | 343/945 [01:11<02:35,  3.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 343/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  36%|███▋      | 344/945 [01:11<02:40,  3.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 344/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  37%|███▋      | 345/945 [01:12<02:40,  3.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 345/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  37%|███▋      | 346/945 [01:12<02:31,  3.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 346/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  37%|███▋      | 347/945 [01:12<02:17,  4.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 347/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  37%|███▋      | 348/945 [01:12<02:10,  4.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 348/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  37%|███▋      | 349/945 [01:12<02:04,  4.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 349/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  37%|███▋      | 350/945 [01:12<01:58,  5.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 350/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  37%|███▋      | 351/945 [01:13<01:56,  5.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 351/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  37%|███▋      | 352/945 [01:13<01:51,  5.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 352/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  37%|███▋      | 353/945 [01:13<01:53,  5.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 353/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  37%|███▋      | 354/945 [01:13<01:53,  5.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 354/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  38%|███▊      | 355/945 [01:13<01:51,  5.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 355/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  38%|███▊      | 356/945 [01:14<01:50,  5.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 356/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  38%|███▊      | 357/945 [01:14<01:48,  5.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 357/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  38%|███▊      | 358/945 [01:14<01:50,  5.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 358/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  38%|███▊      | 359/945 [01:14<01:54,  5.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 359/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  38%|███▊      | 360/945 [01:14<01:52,  5.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 360/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  38%|███▊      | 361/945 [01:15<01:51,  5.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 361/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  38%|███▊      | 362/945 [01:15<01:49,  5.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 362/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  38%|███▊      | 363/945 [01:15<01:48,  5.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 363/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  39%|███▊      | 364/945 [01:15<01:47,  5.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 364/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  39%|███▊      | 365/945 [01:15<01:49,  5.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 365/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  39%|███▊      | 366/945 [01:15<01:46,  5.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 366/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  39%|███▉      | 367/945 [01:16<01:48,  5.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 367/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  39%|███▉      | 368/945 [01:16<01:46,  5.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 368/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  39%|███▉      | 369/945 [01:16<01:47,  5.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 369/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  39%|███▉      | 370/945 [01:16<01:49,  5.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 370/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  39%|███▉      | 371/945 [01:16<01:46,  5.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 371/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  39%|███▉      | 372/945 [01:17<01:49,  5.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 372/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  39%|███▉      | 373/945 [01:17<01:49,  5.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 373/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  40%|███▉      | 374/945 [01:17<01:48,  5.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 374/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  40%|███▉      | 375/945 [01:17<01:48,  5.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 375/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  40%|███▉      | 376/945 [01:17<01:50,  5.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 376/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  40%|███▉      | 377/945 [01:18<01:50,  5.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 377/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  40%|████      | 378/945 [01:18<01:49,  5.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 378/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  40%|████      | 379/945 [01:18<01:48,  5.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 379/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  40%|████      | 380/945 [01:18<01:47,  5.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 380/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  40%|████      | 381/945 [01:18<01:48,  5.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 381/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  40%|████      | 382/945 [01:19<01:47,  5.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 382/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  41%|████      | 383/945 [01:19<01:48,  5.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 383/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  41%|████      | 384/945 [01:19<01:46,  5.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 384/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  41%|████      | 385/945 [01:19<01:43,  5.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 385/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  41%|████      | 386/945 [01:19<01:41,  5.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 386/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  41%|████      | 387/945 [01:19<01:41,  5.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 387/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  41%|████      | 388/945 [01:20<01:39,  5.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 388/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  41%|████      | 389/945 [01:20<01:39,  5.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 389/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  41%|████▏     | 390/945 [01:20<01:38,  5.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 390/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  41%|████▏     | 391/945 [01:20<01:39,  5.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 391/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  41%|████▏     | 392/945 [01:20<01:39,  5.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 392/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  42%|████▏     | 393/945 [01:20<01:38,  5.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 393/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  42%|████▏     | 394/945 [01:21<01:38,  5.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 394/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  42%|████▏     | 395/945 [01:21<01:38,  5.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 395/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  42%|████▏     | 396/945 [01:21<01:39,  5.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 396/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  42%|████▏     | 397/945 [01:21<01:40,  5.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 397/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  42%|████▏     | 398/945 [01:21<01:42,  5.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 398/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  42%|████▏     | 399/945 [01:22<01:42,  5.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 399/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  42%|████▏     | 400/945 [01:22<01:54,  4.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 400/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  42%|████▏     | 401/945 [01:22<02:04,  4.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 401/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  43%|████▎     | 402/945 [01:22<02:14,  4.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 402/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  43%|████▎     | 403/945 [01:23<02:16,  3.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 403/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  43%|████▎     | 404/945 [01:23<02:18,  3.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 404/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  43%|████▎     | 405/945 [01:23<02:16,  3.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 405/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  43%|████▎     | 406/945 [01:23<02:22,  3.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 406/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  43%|████▎     | 407/945 [01:24<02:28,  3.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 407/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  43%|████▎     | 408/945 [01:24<02:28,  3.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 408/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  43%|████▎     | 409/945 [01:24<02:28,  3.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 409/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  43%|████▎     | 410/945 [01:25<02:24,  3.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 410/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  43%|████▎     | 411/945 [01:25<02:20,  3.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 411/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  44%|████▎     | 412/945 [01:25<02:28,  3.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 412/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  44%|████▎     | 413/945 [01:25<02:28,  3.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 413/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  44%|████▍     | 414/945 [01:26<02:30,  3.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 414/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  44%|████▍     | 415/945 [01:26<02:24,  3.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 415/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  44%|████▍     | 416/945 [01:26<02:22,  3.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 416/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  44%|████▍     | 417/945 [01:27<02:22,  3.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 417/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  44%|████▍     | 418/945 [01:27<02:21,  3.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 418/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  44%|████▍     | 419/945 [01:27<02:22,  3.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 419/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  44%|████▍     | 420/945 [01:27<02:19,  3.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 420/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  45%|████▍     | 421/945 [01:28<02:24,  3.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 421/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  45%|████▍     | 422/945 [01:28<02:25,  3.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 422/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  45%|████▍     | 423/945 [01:28<02:22,  3.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 423/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  45%|████▍     | 424/945 [01:28<02:20,  3.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 424/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  45%|████▍     | 425/945 [01:29<02:21,  3.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 425/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  45%|████▌     | 426/945 [01:29<02:21,  3.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 426/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  45%|████▌     | 427/945 [01:29<02:18,  3.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 427/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  45%|████▌     | 428/945 [01:29<02:09,  3.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 428/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  45%|████▌     | 429/945 [01:30<01:59,  4.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 429/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  46%|████▌     | 430/945 [01:30<01:52,  4.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 430/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  46%|████▌     | 431/945 [01:30<01:47,  4.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 431/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  46%|████▌     | 432/945 [01:30<01:44,  4.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 432/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  46%|████▌     | 433/945 [01:30<01:42,  5.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 433/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  46%|████▌     | 434/945 [01:31<01:39,  5.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 434/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  46%|████▌     | 435/945 [01:31<01:36,  5.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 435/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  46%|████▌     | 436/945 [01:31<01:37,  5.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 436/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  46%|████▌     | 437/945 [01:31<01:35,  5.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 437/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  46%|████▋     | 438/945 [01:31<01:35,  5.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 438/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  46%|████▋     | 439/945 [01:32<01:35,  5.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 439/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  47%|████▋     | 440/945 [01:32<01:33,  5.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 440/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  47%|████▋     | 441/945 [01:32<01:33,  5.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 441/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  47%|████▋     | 442/945 [01:32<01:32,  5.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 442/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  47%|████▋     | 443/945 [01:32<01:33,  5.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 443/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  47%|████▋     | 444/945 [01:32<01:34,  5.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 444/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  47%|████▋     | 445/945 [01:33<01:32,  5.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 445/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  47%|████▋     | 446/945 [01:33<01:32,  5.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 446/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  47%|████▋     | 447/945 [01:33<01:34,  5.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 447/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  47%|████▋     | 448/945 [01:33<01:34,  5.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 448/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  48%|████▊     | 449/945 [01:33<01:33,  5.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 449/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  48%|████▊     | 450/945 [01:34<01:33,  5.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 450/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  48%|████▊     | 451/945 [01:34<01:31,  5.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 451/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  48%|████▊     | 452/945 [01:34<01:32,  5.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 452/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  48%|████▊     | 453/945 [01:34<01:32,  5.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 453/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  48%|████▊     | 454/945 [01:34<01:31,  5.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 454/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  48%|████▊     | 455/945 [01:35<01:32,  5.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 455/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  48%|████▊     | 456/945 [01:35<01:32,  5.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 456/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  48%|████▊     | 457/945 [01:35<01:31,  5.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 457/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  48%|████▊     | 458/945 [01:35<01:33,  5.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 458/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  49%|████▊     | 459/945 [01:35<01:33,  5.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 459/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  49%|████▊     | 460/945 [01:35<01:30,  5.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 460/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  49%|████▉     | 461/945 [01:36<01:32,  5.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 461/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  49%|████▉     | 462/945 [01:36<01:34,  5.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 462/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  49%|████▉     | 463/945 [01:36<01:32,  5.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 463/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  49%|████▉     | 464/945 [01:36<01:35,  5.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 464/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  49%|████▉     | 465/945 [01:36<01:36,  4.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 465/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  49%|████▉     | 466/945 [01:37<01:36,  4.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 466/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  49%|████▉     | 467/945 [01:37<01:34,  5.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 467/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  50%|████▉     | 468/945 [01:37<01:33,  5.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 468/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  50%|████▉     | 469/945 [01:37<01:33,  5.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 469/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  50%|████▉     | 470/945 [01:37<01:32,  5.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 470/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  50%|████▉     | 471/945 [01:38<01:33,  5.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 471/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  50%|████▉     | 472/945 [01:38<01:35,  4.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 472/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  50%|█████     | 473/945 [01:38<01:34,  4.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 473/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  50%|█████     | 474/945 [01:38<01:34,  4.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 474/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  50%|█████     | 475/945 [01:38<01:32,  5.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 475/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  50%|█████     | 476/945 [01:39<01:31,  5.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 476/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  50%|█████     | 477/945 [01:39<01:31,  5.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 477/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  51%|█████     | 478/945 [01:39<01:31,  5.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 478/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  51%|█████     | 479/945 [01:39<01:31,  5.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 479/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  51%|█████     | 480/945 [01:39<01:33,  4.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 480/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  51%|█████     | 481/945 [01:40<01:39,  4.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 481/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  51%|█████     | 482/945 [01:40<01:43,  4.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 482/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  51%|█████     | 483/945 [01:40<01:48,  4.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 483/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  51%|█████     | 484/945 [01:40<01:52,  4.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 484/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  51%|█████▏    | 485/945 [01:41<01:53,  4.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 485/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  51%|█████▏    | 486/945 [01:41<01:57,  3.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 486/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  52%|█████▏    | 487/945 [01:41<02:01,  3.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 487/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  52%|█████▏    | 488/945 [01:42<02:05,  3.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 488/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  52%|█████▏    | 489/945 [01:42<02:06,  3.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 489/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  52%|█████▏    | 490/945 [01:42<02:04,  3.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 490/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  52%|█████▏    | 491/945 [01:42<02:01,  3.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 491/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  52%|█████▏    | 492/945 [01:43<02:00,  3.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 492/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  52%|█████▏    | 493/945 [01:43<02:02,  3.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 493/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  52%|█████▏    | 494/945 [01:43<02:03,  3.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 494/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  52%|█████▏    | 495/945 [01:43<02:01,  3.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 495/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  52%|█████▏    | 496/945 [01:44<02:08,  3.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 496/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  53%|█████▎    | 497/945 [01:44<02:07,  3.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 497/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  53%|█████▎    | 498/945 [01:44<02:09,  3.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 498/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  53%|█████▎    | 499/945 [01:45<02:05,  3.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 499/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  53%|█████▎    | 500/945 [01:45<02:04,  3.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 500/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  53%|█████▎    | 501/945 [01:45<02:05,  3.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 501/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  53%|█████▎    | 502/945 [01:45<02:04,  3.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 502/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  53%|█████▎    | 503/945 [01:46<02:05,  3.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 503/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  53%|█████▎    | 504/945 [01:46<02:04,  3.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 504/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  53%|█████▎    | 505/945 [01:46<02:03,  3.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 505/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  54%|█████▎    | 506/945 [01:47<02:03,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 506/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  54%|█████▎    | 507/945 [01:47<01:57,  3.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 507/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  54%|█████▍    | 508/945 [01:47<01:59,  3.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 508/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  54%|█████▍    | 509/945 [01:47<01:57,  3.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 509/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  54%|█████▍    | 510/945 [01:48<01:49,  3.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 510/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  54%|█████▍    | 511/945 [01:48<01:40,  4.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 511/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  54%|█████▍    | 512/945 [01:48<01:33,  4.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 512/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  54%|█████▍    | 513/945 [01:48<01:30,  4.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 513/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  54%|█████▍    | 514/945 [01:48<01:31,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 514/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  54%|█████▍    | 515/945 [01:49<01:29,  4.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 515/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  55%|█████▍    | 516/945 [01:49<01:26,  4.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 516/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  55%|█████▍    | 517/945 [01:49<01:25,  5.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 517/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  55%|█████▍    | 518/945 [01:49<01:24,  5.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 518/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  55%|█████▍    | 519/945 [01:49<01:23,  5.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 519/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  55%|█████▌    | 520/945 [01:50<01:23,  5.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 520/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  55%|█████▌    | 521/945 [01:50<01:23,  5.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 521/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  55%|█████▌    | 522/945 [01:50<01:23,  5.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 522/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  55%|█████▌    | 523/945 [01:50<01:24,  5.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 523/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  55%|█████▌    | 524/945 [01:50<01:23,  5.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 524/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  56%|█████▌    | 525/945 [01:50<01:20,  5.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 525/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  56%|█████▌    | 526/945 [01:51<01:20,  5.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 526/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  56%|█████▌    | 527/945 [01:51<01:19,  5.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 527/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  56%|█████▌    | 528/945 [01:51<01:19,  5.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 528/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  56%|█████▌    | 529/945 [01:51<01:17,  5.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 529/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  56%|█████▌    | 530/945 [01:51<01:17,  5.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 530/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  56%|█████▌    | 531/945 [01:52<01:18,  5.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 531/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  56%|█████▋    | 532/945 [01:52<01:16,  5.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 532/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  56%|█████▋    | 533/945 [01:52<01:18,  5.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 533/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  57%|█████▋    | 534/945 [01:52<01:19,  5.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 534/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  57%|█████▋    | 535/945 [01:52<01:21,  5.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 535/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  57%|█████▋    | 536/945 [01:53<01:21,  5.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 536/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  57%|█████▋    | 537/945 [01:53<01:20,  5.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 537/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  57%|█████▋    | 538/945 [01:53<01:20,  5.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 538/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  57%|█████▋    | 539/945 [01:53<01:17,  5.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 539/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  57%|█████▋    | 540/945 [01:53<01:18,  5.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 540/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  57%|█████▋    | 541/945 [01:54<01:17,  5.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 541/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  57%|█████▋    | 542/945 [01:54<01:15,  5.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 542/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  57%|█████▋    | 543/945 [01:54<01:14,  5.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 543/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  58%|█████▊    | 544/945 [01:54<01:15,  5.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 544/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  58%|█████▊    | 545/945 [01:54<01:15,  5.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 545/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  58%|█████▊    | 546/945 [01:54<01:15,  5.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 546/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  58%|█████▊    | 547/945 [01:55<01:15,  5.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 547/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  58%|█████▊    | 548/945 [01:55<01:15,  5.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 548/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  58%|█████▊    | 549/945 [01:55<01:14,  5.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 549/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  58%|█████▊    | 550/945 [01:55<01:15,  5.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 550/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  58%|█████▊    | 551/945 [01:55<01:14,  5.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 551/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  58%|█████▊    | 552/945 [01:56<01:13,  5.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 552/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  59%|█████▊    | 553/945 [01:56<01:13,  5.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 553/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  59%|█████▊    | 554/945 [01:56<01:12,  5.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 554/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  59%|█████▊    | 555/945 [01:56<01:14,  5.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 555/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  59%|█████▉    | 556/945 [01:56<01:15,  5.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 556/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  59%|█████▉    | 557/945 [01:57<01:14,  5.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 557/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  59%|█████▉    | 558/945 [01:57<01:16,  5.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 558/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  59%|█████▉    | 559/945 [01:57<01:16,  5.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 559/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  59%|█████▉    | 560/945 [01:57<01:16,  5.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 560/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  59%|█████▉    | 561/945 [01:57<01:14,  5.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 561/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  59%|█████▉    | 562/945 [01:58<01:21,  4.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 562/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  60%|█████▉    | 563/945 [01:58<01:30,  4.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 563/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  60%|█████▉    | 564/945 [01:58<01:40,  3.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 564/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  60%|█████▉    | 565/945 [01:59<01:41,  3.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 565/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  60%|█████▉    | 566/945 [01:59<01:43,  3.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 566/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  60%|██████    | 567/945 [01:59<01:43,  3.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 567/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  60%|██████    | 568/945 [01:59<01:45,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 568/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  60%|██████    | 569/945 [02:00<01:45,  3.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 569/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  60%|██████    | 570/945 [02:00<01:46,  3.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 570/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  60%|██████    | 571/945 [02:00<01:46,  3.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 571/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  61%|██████    | 572/945 [02:01<01:44,  3.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 572/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  61%|██████    | 573/945 [02:01<01:47,  3.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 573/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  61%|██████    | 574/945 [02:01<01:47,  3.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 574/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  61%|██████    | 575/945 [02:01<01:49,  3.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 575/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  61%|██████    | 576/945 [02:02<01:47,  3.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 576/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  61%|██████    | 577/945 [02:02<01:44,  3.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 577/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  61%|██████    | 578/945 [02:02<01:43,  3.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 578/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  61%|██████▏   | 579/945 [02:03<01:40,  3.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 579/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  61%|██████▏   | 580/945 [02:03<01:39,  3.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 580/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  61%|██████▏   | 581/945 [02:03<01:37,  3.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 581/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  62%|██████▏   | 582/945 [02:03<01:41,  3.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 582/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  62%|██████▏   | 583/945 [02:04<01:39,  3.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 583/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  62%|██████▏   | 584/945 [02:04<01:38,  3.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 584/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  62%|██████▏   | 585/945 [02:04<01:34,  3.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 585/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  62%|██████▏   | 586/945 [02:04<01:35,  3.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 586/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  62%|██████▏   | 587/945 [02:05<01:37,  3.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 587/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  62%|██████▏   | 588/945 [02:05<01:41,  3.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 588/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  62%|██████▏   | 589/945 [02:05<01:38,  3.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 589/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  62%|██████▏   | 590/945 [02:06<01:37,  3.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 590/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  63%|██████▎   | 591/945 [02:06<01:29,  3.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 591/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  63%|██████▎   | 592/945 [02:06<01:25,  4.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 592/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  63%|██████▎   | 593/945 [02:06<01:18,  4.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 593/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  63%|██████▎   | 594/945 [02:06<01:14,  4.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 594/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  63%|██████▎   | 595/945 [02:06<01:10,  4.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 595/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  63%|██████▎   | 596/945 [02:07<01:11,  4.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 596/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  63%|██████▎   | 597/945 [02:07<01:09,  5.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 597/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  63%|██████▎   | 598/945 [02:07<01:07,  5.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 598/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  63%|██████▎   | 599/945 [02:07<01:08,  5.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 599/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  63%|██████▎   | 600/945 [02:07<01:07,  5.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 600/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  64%|██████▎   | 601/945 [02:08<01:08,  5.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 601/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  64%|██████▎   | 602/945 [02:08<01:07,  5.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 602/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  64%|██████▍   | 603/945 [02:08<01:05,  5.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 603/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  64%|██████▍   | 604/945 [02:08<01:04,  5.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 604/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  64%|██████▍   | 605/945 [02:08<01:05,  5.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 605/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  64%|██████▍   | 606/945 [02:09<01:06,  5.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 606/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  64%|██████▍   | 607/945 [02:09<01:06,  5.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 607/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  64%|██████▍   | 608/945 [02:09<01:05,  5.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 608/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  64%|██████▍   | 609/945 [02:09<01:06,  5.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 609/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  65%|██████▍   | 610/945 [02:09<01:06,  5.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 610/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  65%|██████▍   | 611/945 [02:10<01:05,  5.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 611/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  65%|██████▍   | 612/945 [02:10<01:06,  5.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 612/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  65%|██████▍   | 613/945 [02:10<01:04,  5.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 613/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  65%|██████▍   | 614/945 [02:10<01:05,  5.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 614/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  65%|██████▌   | 615/945 [02:10<01:04,  5.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 615/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  65%|██████▌   | 616/945 [02:11<01:03,  5.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 616/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 617/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Batches:  65%|██████▌   | 617/945 [02:11<01:03,  5.14it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  65%|██████▌   | 618/945 [02:11<01:06,  4.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 618/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  66%|██████▌   | 619/945 [02:11<01:04,  5.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 619/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  66%|██████▌   | 620/945 [02:11<01:03,  5.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 620/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  66%|██████▌   | 621/945 [02:12<01:02,  5.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 621/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  66%|██████▌   | 622/945 [02:12<01:02,  5.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 622/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  66%|██████▌   | 623/945 [02:12<01:04,  5.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 623/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  66%|██████▌   | 624/945 [02:12<01:04,  4.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 624/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  66%|██████▌   | 625/945 [02:12<01:04,  4.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 625/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  66%|██████▌   | 626/945 [02:13<01:04,  4.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 626/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  66%|██████▋   | 627/945 [02:13<01:05,  4.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 627/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  66%|██████▋   | 628/945 [02:13<01:06,  4.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 628/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  67%|██████▋   | 629/945 [02:13<01:03,  4.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 629/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 630/945"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Batches:  67%|██████▋   | 630/945 [02:13<01:03,  4.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  67%|██████▋   | 631/945 [02:14<01:03,  4.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 631/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  67%|██████▋   | 632/945 [02:14<01:02,  4.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 632/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 633/945"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Batches:  67%|██████▋   | 633/945 [02:14<01:02,  4.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  67%|██████▋   | 634/945 [02:14<01:02,  4.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 634/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  67%|██████▋   | 635/945 [02:14<01:01,  5.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 635/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  67%|██████▋   | 636/945 [02:15<01:00,  5.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 636/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  67%|██████▋   | 637/945 [02:15<01:00,  5.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 637/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  68%|██████▊   | 638/945 [02:15<01:02,  4.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 638/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  68%|██████▊   | 639/945 [02:15<01:00,  5.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 639/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  68%|██████▊   | 640/945 [02:15<01:00,  5.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 640/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  68%|██████▊   | 641/945 [02:16<01:00,  5.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 641/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  68%|██████▊   | 642/945 [02:16<01:05,  4.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 642/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  68%|██████▊   | 643/945 [02:16<01:11,  4.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 643/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  68%|██████▊   | 644/945 [02:16<01:17,  3.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 644/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  68%|██████▊   | 645/945 [02:17<01:21,  3.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 645/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  68%|██████▊   | 646/945 [02:17<01:24,  3.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 646/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  68%|██████▊   | 647/945 [02:17<01:25,  3.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 647/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  69%|██████▊   | 648/945 [02:18<01:24,  3.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 648/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  69%|██████▊   | 649/945 [02:18<01:24,  3.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 649/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  69%|██████▉   | 650/945 [02:18<01:27,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 650/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  69%|██████▉   | 651/945 [02:19<01:27,  3.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 651/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  69%|██████▉   | 652/945 [02:19<01:24,  3.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 652/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  69%|██████▉   | 653/945 [02:19<01:24,  3.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 653/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  69%|██████▉   | 654/945 [02:19<01:25,  3.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 654/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  69%|██████▉   | 655/945 [02:20<01:23,  3.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 655/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  69%|██████▉   | 656/945 [02:20<01:25,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 656/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  70%|██████▉   | 657/945 [02:20<01:23,  3.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 657/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  70%|██████▉   | 658/945 [02:21<01:26,  3.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 658/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  70%|██████▉   | 659/945 [02:21<01:26,  3.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 659/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  70%|██████▉   | 660/945 [02:21<01:24,  3.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 660/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  70%|██████▉   | 661/945 [02:21<01:24,  3.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 661/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  70%|███████   | 662/945 [02:22<01:25,  3.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 662/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  70%|███████   | 663/945 [02:22<01:24,  3.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 663/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  70%|███████   | 664/945 [02:22<01:27,  3.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 664/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  70%|███████   | 665/945 [02:23<01:26,  3.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 665/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  70%|███████   | 666/945 [02:23<01:22,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 666/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  71%|███████   | 667/945 [02:23<01:22,  3.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 667/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  71%|███████   | 668/945 [02:24<01:23,  3.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 668/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  71%|███████   | 669/945 [02:24<01:16,  3.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 669/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  71%|███████   | 670/945 [02:24<01:10,  3.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 670/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  71%|███████   | 671/945 [02:24<01:06,  4.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 671/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  71%|███████   | 672/945 [02:24<01:02,  4.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 672/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  71%|███████   | 673/945 [02:25<01:01,  4.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 673/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  71%|███████▏  | 674/945 [02:25<00:59,  4.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 674/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  71%|███████▏  | 675/945 [02:25<00:57,  4.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 675/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  72%|███████▏  | 676/945 [02:25<00:57,  4.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 676/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  72%|███████▏  | 677/945 [02:25<00:56,  4.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 677/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  72%|███████▏  | 678/945 [02:26<00:55,  4.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 678/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  72%|███████▏  | 679/945 [02:26<00:54,  4.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 679/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  72%|███████▏  | 680/945 [02:26<00:53,  4.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 680/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  72%|███████▏  | 681/945 [02:26<00:52,  4.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 681/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  72%|███████▏  | 682/945 [02:26<00:52,  5.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 682/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  72%|███████▏  | 683/945 [02:27<00:53,  4.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 683/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  72%|███████▏  | 684/945 [02:27<00:53,  4.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 684/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  72%|███████▏  | 685/945 [02:27<00:53,  4.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 685/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  73%|███████▎  | 686/945 [02:27<00:53,  4.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 686/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  73%|███████▎  | 687/945 [02:28<00:54,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 687/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  73%|███████▎  | 688/945 [02:28<00:55,  4.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 688/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  73%|███████▎  | 689/945 [02:28<00:51,  4.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 689/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  73%|███████▎  | 690/945 [02:28<00:54,  4.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 690/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  73%|███████▎  | 691/945 [02:28<00:53,  4.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 691/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  73%|███████▎  | 692/945 [02:29<00:52,  4.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 692/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  73%|███████▎  | 693/945 [02:29<00:54,  4.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 693/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  73%|███████▎  | 694/945 [02:29<00:53,  4.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 694/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  74%|███████▎  | 695/945 [02:29<00:52,  4.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 695/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  74%|███████▎  | 696/945 [02:29<00:51,  4.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 696/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  74%|███████▍  | 697/945 [02:30<00:52,  4.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 697/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  74%|███████▍  | 698/945 [02:30<00:53,  4.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 698/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  74%|███████▍  | 699/945 [02:30<00:52,  4.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 699/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  74%|███████▍  | 700/945 [02:30<00:50,  4.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 700/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  74%|███████▍  | 701/945 [02:30<00:49,  4.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 701/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  74%|███████▍  | 702/945 [02:31<00:51,  4.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 702/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  74%|███████▍  | 703/945 [02:31<00:52,  4.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 703/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  74%|███████▍  | 704/945 [02:31<00:51,  4.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 704/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  75%|███████▍  | 705/945 [02:31<00:50,  4.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 705/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  75%|███████▍  | 706/945 [02:32<00:50,  4.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 706/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  75%|███████▍  | 707/945 [02:32<00:49,  4.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 707/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  75%|███████▍  | 708/945 [02:32<00:51,  4.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 708/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  75%|███████▌  | 709/945 [02:32<00:51,  4.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 709/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  75%|███████▌  | 710/945 [02:32<00:50,  4.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 710/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  75%|███████▌  | 711/945 [02:33<00:49,  4.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 711/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  75%|███████▌  | 712/945 [02:33<00:51,  4.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 712/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  75%|███████▌  | 713/945 [02:33<00:49,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 713/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  76%|███████▌  | 714/945 [02:33<00:48,  4.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 714/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  76%|███████▌  | 715/945 [02:33<00:47,  4.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 715/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  76%|███████▌  | 716/945 [02:34<00:57,  3.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 716/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  76%|███████▌  | 717/945 [02:34<01:14,  3.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 717/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  76%|███████▌  | 718/945 [02:35<01:22,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 718/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  76%|███████▌  | 719/945 [02:35<01:27,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 719/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  76%|███████▌  | 720/945 [02:36<01:29,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 720/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  76%|███████▋  | 721/945 [02:36<01:35,  2.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 721/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  76%|███████▋  | 722/945 [02:37<01:44,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 722/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  77%|███████▋  | 723/945 [02:37<01:42,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 723/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  77%|███████▋  | 724/945 [02:38<01:45,  2.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 724/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  77%|███████▋  | 725/945 [02:38<01:46,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 725/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  77%|███████▋  | 726/945 [02:39<01:46,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 726/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  77%|███████▋  | 727/945 [02:39<01:54,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 727/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  77%|███████▋  | 728/945 [02:40<01:48,  2.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 728/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  77%|███████▋  | 729/945 [02:40<01:44,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 729/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  77%|███████▋  | 730/945 [02:41<02:02,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 730/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  77%|███████▋  | 731/945 [02:42<02:23,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 731/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  77%|███████▋  | 732/945 [02:42<02:22,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 732/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  78%|███████▊  | 733/945 [02:43<02:01,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 733/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  78%|███████▊  | 734/945 [02:43<01:42,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 734/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  78%|███████▊  | 735/945 [02:43<01:26,  2.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 735/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  78%|███████▊  | 736/945 [02:44<01:14,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 736/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  78%|███████▊  | 737/945 [02:44<01:05,  3.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 737/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  78%|███████▊  | 738/945 [02:44<00:58,  3.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 738/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  78%|███████▊  | 739/945 [02:44<00:54,  3.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 739/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  78%|███████▊  | 740/945 [02:45<00:57,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 740/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  78%|███████▊  | 741/945 [02:45<00:59,  3.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 741/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  79%|███████▊  | 742/945 [02:45<00:59,  3.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 742/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  79%|███████▊  | 743/945 [02:45<01:00,  3.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 743/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  79%|███████▊  | 744/945 [02:46<00:58,  3.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 744/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  79%|███████▉  | 745/945 [02:46<00:54,  3.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 745/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  79%|███████▉  | 746/945 [02:46<00:50,  3.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 746/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  79%|███████▉  | 747/945 [02:46<00:46,  4.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 747/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  79%|███████▉  | 748/945 [02:47<00:44,  4.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 748/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  79%|███████▉  | 749/945 [02:47<00:44,  4.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 749/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  79%|███████▉  | 750/945 [02:47<00:42,  4.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 750/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  79%|███████▉  | 751/945 [02:47<00:43,  4.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 751/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  80%|███████▉  | 752/945 [02:47<00:42,  4.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 752/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  80%|███████▉  | 753/945 [02:48<00:41,  4.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 753/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  80%|███████▉  | 754/945 [02:48<00:41,  4.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 754/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  80%|███████▉  | 755/945 [02:48<00:40,  4.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 755/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  80%|████████  | 756/945 [02:48<00:38,  4.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 756/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  80%|████████  | 757/945 [02:48<00:39,  4.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 757/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  80%|████████  | 758/945 [02:49<00:39,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 758/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  80%|████████  | 759/945 [02:49<00:39,  4.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 759/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  80%|████████  | 760/945 [02:49<00:38,  4.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 760/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  81%|████████  | 761/945 [02:49<00:38,  4.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 761/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  81%|████████  | 762/945 [02:50<00:38,  4.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 762/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  81%|████████  | 763/945 [02:50<00:37,  4.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 763/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  81%|████████  | 764/945 [02:50<00:37,  4.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 764/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  81%|████████  | 765/945 [02:50<00:36,  4.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 765/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  81%|████████  | 766/945 [02:50<00:36,  4.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 766/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  81%|████████  | 767/945 [02:51<00:37,  4.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 767/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  81%|████████▏ | 768/945 [02:51<00:37,  4.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 768/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  81%|████████▏ | 769/945 [02:51<00:38,  4.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 769/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  81%|████████▏ | 770/945 [02:51<00:37,  4.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 770/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  82%|████████▏ | 771/945 [02:51<00:35,  4.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 771/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  82%|████████▏ | 772/945 [02:52<00:35,  4.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 772/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  82%|████████▏ | 773/945 [02:52<00:34,  4.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 773/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  82%|████████▏ | 774/945 [02:52<00:35,  4.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 774/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  82%|████████▏ | 775/945 [02:52<00:35,  4.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 775/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  82%|████████▏ | 776/945 [02:52<00:34,  4.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 776/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  82%|████████▏ | 777/945 [02:53<00:34,  4.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 777/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  82%|████████▏ | 778/945 [02:53<00:37,  4.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 778/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  82%|████████▏ | 779/945 [02:53<00:41,  4.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 779/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  83%|████████▎ | 780/945 [02:54<00:44,  3.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 780/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  83%|████████▎ | 781/945 [02:54<00:43,  3.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 781/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  83%|████████▎ | 782/945 [02:54<00:45,  3.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 782/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  83%|████████▎ | 783/945 [02:54<00:46,  3.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 783/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  83%|████████▎ | 784/945 [02:55<00:46,  3.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 784/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  83%|████████▎ | 785/945 [02:55<00:48,  3.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 785/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  83%|████████▎ | 786/945 [02:55<00:48,  3.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 786/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  83%|████████▎ | 787/945 [02:56<00:46,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 787/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  83%|████████▎ | 788/945 [02:56<00:45,  3.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 788/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  83%|████████▎ | 789/945 [02:56<00:43,  3.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 789/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  84%|████████▎ | 790/945 [02:56<00:43,  3.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 790/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  84%|████████▎ | 791/945 [02:57<00:42,  3.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 791/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  84%|████████▍ | 792/945 [02:57<00:41,  3.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 792/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  84%|████████▍ | 793/945 [02:57<00:41,  3.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 793/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  84%|████████▍ | 794/945 [02:58<00:43,  3.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 794/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  84%|████████▍ | 795/945 [02:58<00:44,  3.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 795/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  84%|████████▍ | 796/945 [02:58<00:45,  3.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 796/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  84%|████████▍ | 797/945 [02:58<00:44,  3.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 797/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  84%|████████▍ | 798/945 [02:59<00:42,  3.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 798/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  85%|████████▍ | 799/945 [02:59<00:42,  3.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 799/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  85%|████████▍ | 800/945 [02:59<00:43,  3.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 800/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  85%|████████▍ | 801/945 [03:00<00:42,  3.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 801/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  85%|████████▍ | 802/945 [03:00<00:41,  3.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 802/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  85%|████████▍ | 803/945 [03:00<00:41,  3.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 803/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  85%|████████▌ | 804/945 [03:00<00:40,  3.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 804/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  85%|████████▌ | 805/945 [03:01<00:39,  3.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 805/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  85%|████████▌ | 806/945 [03:01<00:38,  3.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 806/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  85%|████████▌ | 807/945 [03:01<00:37,  3.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 807/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  86%|████████▌ | 808/945 [03:02<00:37,  3.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 808/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  86%|████████▌ | 809/945 [03:02<00:33,  4.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 809/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  86%|████████▌ | 810/945 [03:02<00:31,  4.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 810/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  86%|████████▌ | 811/945 [03:02<00:30,  4.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 811/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  86%|████████▌ | 812/945 [03:02<00:29,  4.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 812/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  86%|████████▌ | 813/945 [03:03<00:28,  4.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 813/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  86%|████████▌ | 814/945 [03:03<00:27,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 814/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  86%|████████▌ | 815/945 [03:03<00:27,  4.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 815/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  86%|████████▋ | 816/945 [03:03<00:27,  4.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 816/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  86%|████████▋ | 817/945 [03:03<00:26,  4.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 817/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  87%|████████▋ | 818/945 [03:04<00:26,  4.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 818/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  87%|████████▋ | 819/945 [03:04<00:26,  4.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 819/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  87%|████████▋ | 820/945 [03:04<00:27,  4.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 820/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  87%|████████▋ | 821/945 [03:04<00:26,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 821/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  87%|████████▋ | 822/945 [03:04<00:26,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 822/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  87%|████████▋ | 823/945 [03:05<00:25,  4.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 823/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  87%|████████▋ | 824/945 [03:05<00:25,  4.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 824/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  87%|████████▋ | 825/945 [03:05<00:25,  4.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 825/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  87%|████████▋ | 826/945 [03:05<00:24,  4.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 826/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  88%|████████▊ | 827/945 [03:06<00:24,  4.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 827/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  88%|████████▊ | 828/945 [03:06<00:24,  4.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 828/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  88%|████████▊ | 829/945 [03:06<00:24,  4.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 829/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  88%|████████▊ | 830/945 [03:06<00:24,  4.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 830/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  88%|████████▊ | 831/945 [03:06<00:24,  4.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 831/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  88%|████████▊ | 832/945 [03:07<00:24,  4.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 832/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  88%|████████▊ | 833/945 [03:07<00:23,  4.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 833/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  88%|████████▊ | 834/945 [03:07<00:23,  4.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 834/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  88%|████████▊ | 835/945 [03:07<00:23,  4.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 835/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  88%|████████▊ | 836/945 [03:07<00:23,  4.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 836/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  89%|████████▊ | 837/945 [03:08<00:22,  4.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 837/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  89%|████████▊ | 838/945 [03:08<00:22,  4.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 838/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  89%|████████▉ | 839/945 [03:08<00:22,  4.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 839/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  89%|████████▉ | 840/945 [03:08<00:21,  4.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 840/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  89%|████████▉ | 841/945 [03:08<00:21,  4.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 841/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  89%|████████▉ | 842/945 [03:09<00:21,  4.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 842/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  89%|████████▉ | 843/945 [03:09<00:21,  4.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 843/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  89%|████████▉ | 844/945 [03:09<00:21,  4.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 844/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  89%|████████▉ | 845/945 [03:09<00:21,  4.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 845/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  90%|████████▉ | 846/945 [03:09<00:20,  4.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 846/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  90%|████████▉ | 847/945 [03:10<00:20,  4.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 847/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  90%|████████▉ | 848/945 [03:10<00:19,  4.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 848/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  90%|████████▉ | 849/945 [03:10<00:18,  5.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 849/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  90%|████████▉ | 850/945 [03:10<00:18,  5.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 850/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  90%|█████████ | 851/945 [03:10<00:18,  4.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 851/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  90%|█████████ | 852/945 [03:11<00:18,  5.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 852/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  90%|█████████ | 853/945 [03:11<00:18,  4.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 853/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  90%|█████████ | 854/945 [03:11<00:19,  4.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 854/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  90%|█████████ | 855/945 [03:11<00:18,  4.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 855/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  91%|█████████ | 856/945 [03:12<00:18,  4.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 856/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  91%|█████████ | 857/945 [03:12<00:19,  4.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 857/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  91%|█████████ | 858/945 [03:12<00:20,  4.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 858/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  91%|█████████ | 859/945 [03:12<00:23,  3.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 859/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  91%|█████████ | 860/945 [03:13<00:23,  3.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 860/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  91%|█████████ | 861/945 [03:13<00:24,  3.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 861/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  91%|█████████ | 862/945 [03:13<00:23,  3.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 862/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  91%|█████████▏| 863/945 [03:14<00:23,  3.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 863/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  91%|█████████▏| 864/945 [03:14<00:23,  3.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 864/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  92%|█████████▏| 865/945 [03:14<00:23,  3.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 865/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  92%|█████████▏| 866/945 [03:14<00:22,  3.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 866/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  92%|█████████▏| 867/945 [03:15<00:22,  3.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 867/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  92%|█████████▏| 868/945 [03:15<00:21,  3.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 868/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  92%|█████████▏| 869/945 [03:15<00:21,  3.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 869/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  92%|█████████▏| 870/945 [03:16<00:21,  3.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 870/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  92%|█████████▏| 871/945 [03:16<00:21,  3.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 871/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  92%|█████████▏| 872/945 [03:16<00:21,  3.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 872/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  92%|█████████▏| 873/945 [03:16<00:21,  3.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 873/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  92%|█████████▏| 874/945 [03:17<00:20,  3.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 874/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  93%|█████████▎| 875/945 [03:17<00:21,  3.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 875/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  93%|█████████▎| 876/945 [03:17<00:20,  3.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 876/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  93%|█████████▎| 877/945 [03:18<00:20,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 877/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  93%|█████████▎| 878/945 [03:18<00:19,  3.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 878/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  93%|█████████▎| 879/945 [03:18<00:18,  3.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 879/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  93%|█████████▎| 880/945 [03:19<00:18,  3.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 880/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  93%|█████████▎| 881/945 [03:19<00:18,  3.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 881/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  93%|█████████▎| 882/945 [03:19<00:18,  3.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 882/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  93%|█████████▎| 883/945 [03:19<00:18,  3.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 883/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  94%|█████████▎| 884/945 [03:20<00:17,  3.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 884/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  94%|█████████▎| 885/945 [03:20<00:17,  3.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 885/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  94%|█████████▍| 886/945 [03:20<00:16,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 886/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  94%|█████████▍| 887/945 [03:21<00:15,  3.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 887/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  94%|█████████▍| 888/945 [03:21<00:14,  3.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 888/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  94%|█████████▍| 889/945 [03:21<00:13,  4.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 889/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  94%|█████████▍| 890/945 [03:21<00:12,  4.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 890/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  94%|█████████▍| 891/945 [03:21<00:12,  4.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 891/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  94%|█████████▍| 892/945 [03:22<00:12,  4.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 892/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  94%|█████████▍| 893/945 [03:22<00:11,  4.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 893/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  95%|█████████▍| 894/945 [03:22<00:11,  4.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 894/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  95%|█████████▍| 895/945 [03:22<00:11,  4.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 895/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  95%|█████████▍| 896/945 [03:22<00:11,  4.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 896/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  95%|█████████▍| 897/945 [03:23<00:10,  4.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 897/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  95%|█████████▌| 898/945 [03:23<00:10,  4.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 898/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  95%|█████████▌| 899/945 [03:23<00:09,  4.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 899/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  95%|█████████▌| 900/945 [03:23<00:10,  4.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 900/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  95%|█████████▌| 901/945 [03:24<00:09,  4.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 901/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  95%|█████████▌| 902/945 [03:24<00:09,  4.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 902/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  96%|█████████▌| 903/945 [03:24<00:09,  4.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 903/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  96%|█████████▌| 904/945 [03:24<00:08,  4.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 904/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 905/945"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Batches:  96%|█████████▌| 905/945 [03:24<00:08,  4.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  96%|█████████▌| 906/945 [03:25<00:08,  4.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 906/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  96%|█████████▌| 907/945 [03:25<00:08,  4.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 907/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  96%|█████████▌| 908/945 [03:25<00:08,  4.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 908/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  96%|█████████▌| 909/945 [03:25<00:07,  4.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 909/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  96%|█████████▋| 910/945 [03:26<00:07,  4.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 910/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  96%|█████████▋| 911/945 [03:26<00:07,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 911/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  97%|█████████▋| 912/945 [03:26<00:07,  4.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 912/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  97%|█████████▋| 913/945 [03:26<00:06,  4.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 913/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  97%|█████████▋| 914/945 [03:26<00:06,  4.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 914/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  97%|█████████▋| 915/945 [03:27<00:06,  4.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 915/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  97%|█████████▋| 916/945 [03:27<00:06,  4.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 916/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  97%|█████████▋| 917/945 [03:27<00:05,  4.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 917/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  97%|█████████▋| 918/945 [03:27<00:05,  4.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 918/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  97%|█████████▋| 919/945 [03:27<00:05,  4.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 919/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  97%|█████████▋| 920/945 [03:28<00:05,  4.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 920/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  97%|█████████▋| 921/945 [03:28<00:05,  4.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 921/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  98%|█████████▊| 922/945 [03:28<00:04,  4.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 922/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  98%|█████████▊| 923/945 [03:28<00:04,  4.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 923/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  98%|█████████▊| 924/945 [03:29<00:04,  4.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 924/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  98%|█████████▊| 925/945 [03:29<00:04,  4.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 925/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  98%|█████████▊| 926/945 [03:29<00:04,  4.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 926/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  98%|█████████▊| 927/945 [03:29<00:03,  4.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 927/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  98%|█████████▊| 928/945 [03:29<00:03,  4.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 928/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  98%|█████████▊| 929/945 [03:30<00:03,  4.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 929/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  98%|█████████▊| 930/945 [03:30<00:03,  4.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 930/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  99%|█████████▊| 931/945 [03:30<00:02,  4.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 931/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  99%|█████████▊| 932/945 [03:30<00:02,  4.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 932/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  99%|█████████▊| 933/945 [03:30<00:02,  4.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 933/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  99%|█████████▉| 934/945 [03:31<00:02,  4.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 934/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  99%|█████████▉| 935/945 [03:31<00:02,  3.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 935/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  99%|█████████▉| 936/945 [03:31<00:02,  3.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 936/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  99%|█████████▉| 937/945 [03:32<00:02,  3.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 937/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  99%|█████████▉| 938/945 [03:32<00:02,  3.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 938/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  99%|█████████▉| 939/945 [03:32<00:01,  3.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 939/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches:  99%|█████████▉| 940/945 [03:33<00:01,  3.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 940/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches: 100%|█████████▉| 941/945 [03:33<00:01,  3.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 941/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches: 100%|█████████▉| 942/945 [03:33<00:00,  3.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 942/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches: 100%|█████████▉| 943/945 [03:33<00:00,  3.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 943/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training Batches: 100%|█████████▉| 944/945 [03:34<00:00,  3.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 944/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "                                                                   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 945/945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Assuming train_DataLoader is your DataLoader instance\n",
        "\n",
        "# Wrap train_DataLoader with tqdm for the loading bar\n",
        "for batch_idx, Roberta_data in enumerate(tqdm(train_DataLoader, desc=\"Training Batches\", leave=False)):\n",
        "\n",
        "    Roberta_data=next(iter(train_DataLoader))\n",
        "    Roberta_data.keys()\n",
        "\n",
        "    # Print some information about the current batch\n",
        "    print(f\"Processing batch {batch_idx + 1}/{len(train_DataLoader)}\")\n",
        "\n",
        "    # Access specific elements in the batch using the keys\n",
        "    input_ids = Roberta_data['input_ids']\n",
        "    attention_mask = Roberta_data['attention_mask']\n",
        "    labels = Roberta_data['label'].shape\n",
        "\n",
        "# Training loop complete\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "c729e118",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c729e118",
        "outputId": "ec769a3b-ca6e-4e44-e182-38e88d08202f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['text', 'input_ids', 'attention_mask', 'label'])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "Roberta_data=next(iter(train_DataLoader))\n",
        "Roberta_data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "8596d9db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8596d9db",
        "outputId": "1e544871-ac45-49e4-cb04-59c8107fb548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the XLNet_data keys...\n",
            "Input_ids : torch.Size([8, 140])\n",
            "Attention_mask : torch.Size([8, 140])\n",
            "targets : torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "print('Shape of the XLNet_data keys...')\n",
        "print(f\"Input_ids : {Roberta_data['input_ids'].shape}\")\n",
        "print(f\"Attention_mask : {Roberta_data['attention_mask'].shape}\")\n",
        "print(f\"targets : {Roberta_data['label'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda:0')\n",
        "torch.cuda.get_device_name(0)\n",
        "\n",
        "input_ids = Roberta_data['input_ids'].to(device)\n",
        "attention_mask = Roberta_data['attention_mask'].to(device)\n",
        "targets=Roberta_data['label'].to(device)\n",
        "print(input_ids.shape)      # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HeXiVFuqGm3",
        "outputId": "3a5ee3db-fe5c-44a5-fad3-8dc6b96ec29b"
      },
      "id": "7HeXiVFuqGm3",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 140])\n",
            "torch.Size([8, 140])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UERLbu3GqLvw"
      },
      "id": "UERLbu3GqLvw",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now build a  Classifier\n",
        "Let's load basic Roberta model"
      ],
      "metadata": {
        "id": "3PBHUY82qcNo"
      },
      "id": "3PBHUY82qcNo"
    },
    {
      "cell_type": "code",
      "source": [
        "Roberta_model = RobertaModel.from_pretrained(pre_trained_Robertamodel,return_dict=False)\n",
        "Roberta_model=Roberta_model.to(device)"
      ],
      "metadata": {
        "id": "dLmc26WkqgO8"
      },
      "id": "dLmc26WkqgO8",
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes=3"
      ],
      "metadata": {
        "id": "j3ki3UcoqhuB"
      },
      "id": "j3ki3UcoqhuB",
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Roberta_DepressionClassifier(nn.Module):\n",
        "   def __init__(self, n_classes):\n",
        "     super(Roberta_DepressionClassifier, self).__init__()\n",
        "\n",
        "     self.roberta = RobertaModel.from_pretrained(pre_trained_Robertamodel, return_dict=False)\n",
        "     self.drop = nn.Dropout(p=0.35)\n",
        "     self.hidden=nn.Linear(self.roberta.config.hidden_size,128)\n",
        "     self.out = nn.Linear(128, n_classes)\n",
        "\n",
        "    #  self.softmax=nn.Softmax(dim=1)\n",
        "   def forward(self, input_ids, attention_mask):\n",
        "    last_hidden_states, pooled_output = self.roberta(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    output = self.hidden(output)\n",
        "    # output = self.out(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "mJFs_fRqqlVA"
      },
      "id": "mJFs_fRqqlVA",
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pooled_output is the summary of the tweet. We are using a dropout layer for some regularization and 2 fully-connected layers for our output."
      ],
      "metadata": {
        "id": "WTxmELdpq18U"
      },
      "id": "WTxmELdpq18U"
    },
    {
      "cell_type": "code",
      "source": [
        "Roberta_model = Roberta_DepressionClassifier(len(class_names))\n",
        "Roberta_model=Roberta_model.to(device)"
      ],
      "metadata": {
        "id": "JOFIRQ5Aq0JK"
      },
      "id": "JOFIRQ5Aq0JK",
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'input id {input_ids}')\n",
        "print(f'attention mask {attention_mask}')\n",
        "\n",
        "# Pass the tensors to the RoBERTa model\n",
        "roberta_output = Roberta_model(input_ids, attention_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2Z5e25gtEBp",
        "outputId": "fb86e72a-08ab-4cbc-c6b4-6552c4f342c8"
      },
      "id": "G2Z5e25gtEBp",
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input id tensor([[    0, 24837,    16,  ...,   120,    81,     2],\n",
            "        [    0,  6209,    24,  ...,     1,     1,     1],\n",
            "        [    0,   100,   437,  ...,   114,    38,     2],\n",
            "        ...,\n",
            "        [    0,   118,   218,  ...,     1,     1,     1],\n",
            "        [    0,   495,  2533,  ...,  1010,     7,     2],\n",
            "        [    0, 20470,  1916,  ...,     1,     1,     1]], device='cuda:0')\n",
            "attention mask tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply softmax to the output\n",
        "softmax_output = F.softmax(roberta_output, dim=1).to(device)"
      ],
      "metadata": {
        "id": "aDj7UnF3q3nq"
      },
      "id": "aDj7UnF3q3nq",
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-f--Q-jq8N7",
        "outputId": "0b5c0779-fa62-49b3-cf18-f6aaeeb69e3a"
      },
      "id": "u-f--Q-jq8N7",
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0055, 0.0115, 0.0086,  ..., 0.0093, 0.0076, 0.0088],\n",
              "        [0.0064, 0.0112, 0.0081,  ..., 0.0092, 0.0086, 0.0073],\n",
              "        [0.0069, 0.0096, 0.0084,  ..., 0.0105, 0.0076, 0.0069],\n",
              "        ...,\n",
              "        [0.0056, 0.0135, 0.0070,  ..., 0.0098, 0.0070, 0.0092],\n",
              "        [0.0064, 0.0086, 0.0077,  ..., 0.0111, 0.0077, 0.0077],\n",
              "        [0.0065, 0.0103, 0.0069,  ..., 0.0104, 0.0096, 0.0074]],\n",
              "       device='cuda:0', grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train our Roberta Sentiment classifier\n",
        "\n",
        "We will use AdamW optimizer for correcting weight decay. We will also use, linear scheduler with no warm up steps:\n"
      ],
      "metadata": {
        "id": "x4PXrq3p0H95"
      },
      "id": "x4PXrq3p0H95"
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=5\n",
        "optimizer=AdamW(Roberta_model.parameters(),lr=2e-5,correct_bias=False)\n",
        "total_steps=len(train_DataLoader)*epochs\n",
        "\n",
        "scheduler=get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn=nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "id": "j3GJsupX0DQo"
      },
      "id": "j3GJsupX0DQo",
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPVp51mY0KIX",
        "outputId": "0a8481d4-51ca-4008-d771-c5cf793ab4c8"
      },
      "id": "PPVp51mY0KIX",
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdamW (\n",
              "Parameter Group 0\n",
              "    betas: (0.9, 0.999)\n",
              "    correct_bias: False\n",
              "    eps: 1e-06\n",
              "    initial_lr: 2e-05\n",
              "    lr: 2e-05\n",
              "    weight_decay: 0.0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  device,\n",
        "  scheduler,\n",
        "  n_observations\n",
        "):\n",
        "  model = model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"label\"].to(device)\n",
        "    #Feed data to BERT model\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "      )\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)     # Clip gradients to avoid exploding gradient problem\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "  return correct_predictions.double() / n_observations, np.mean(losses)"
      ],
      "metadata": {
        "id": "SUZkI6940N0n"
      },
      "id": "SUZkI6940N0n",
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, data_loader,device,loss_fn, n_observations):\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"label\"].to(device)\n",
        "      # Feed data to BERT model\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      loss = loss_fn(outputs, targets)\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "  return correct_predictions.double() / n_observations, np.mean(losses)\n"
      ],
      "metadata": {
        "id": "zBWdgWEH0VUA"
      },
      "id": "zBWdgWEH0VUA",
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "kWj8kQMv0krc"
      },
      "id": "kWj8kQMv0krc",
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap your DataLoader with tqdm for a progress bar\n",
        "train_DataLoader = tqdm(train_DataLoader, total=len(train_DataLoader), desc=\"Training\")\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{epochs}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train(\n",
        "    Roberta_model,\n",
        "    train_DataLoader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    len(df_train)\n",
        "  )\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "  val_acc, val_loss = eval_model(\n",
        "    Roberta_model,\n",
        "    valid_DataLoader,\n",
        "    device,\n",
        "    loss_fn,\n",
        "    len(df_val)\n",
        "  )\n",
        "  print(f'Validation  loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  # Close the tqdm progress bar after each epoch\n",
        "  train_DataLoader.close()\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(Roberta_model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqRW33xK0YFv",
        "outputId": "f0e019ff-69c0-49d9-dc7c-9f4fc40f7ee2"
      },
      "id": "lqRW33xK0YFv",
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/945 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Training: 100%|██████████| 945/945 [04:57<00:00,  3.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.5216999753935154 accuracy 0.8365753605928279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation  loss 0.3720114379005875 accuracy 0.8800599700149925\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.3649564944922175 accuracy 0.9039301310043668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation  loss 0.5092653518035161 accuracy 0.8838080959520239\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.2591967401553005 accuracy 0.9343654889506419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation  loss 0.4959085995302128 accuracy 0.8920539730134932\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.18615330120286208 accuracy 0.9548762736535663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation  loss 0.5405116423910127 accuracy 0.8980509745127436\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.15858596744515968 accuracy 0.9644038639671828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation  loss 0.5600522394298331 accuracy 0.896551724137931\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's look at training vs validation accuracy:\n",
        "# Move tensors to CPU before plotting\n",
        "train_acc_cpu = [acc.item() for acc in history['train_acc']]\n",
        "val_acc_cpu = [acc.item() for acc in history['val_acc']]\n",
        "\n",
        "# Plot training vs validation accuracy\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(train_acc_cpu, label='train accuracy')\n",
        "plt.plot(val_acc_cpu, label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "uS8f6wqu0aWQ",
        "outputId": "af8ec87a-8f89-4911-8303-d258fe032b6e"
      },
      "id": "uS8f6wqu0aWQ",
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation\n",
        "dev_acc, _ = eval_model(\n",
        "  Roberta_model,\n",
        "  dev_DataLoader,\n",
        "  device,\n",
        "  loss_fn,\n",
        "  len(dev_df)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3ILV3q041kd",
        "outputId": "f22701b4-77d3-4508-e591-84168f471a0d"
      },
      "id": "z3ILV3q041kd",
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_acc.item()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kzGPsvd6ZiB",
        "outputId": "00671ca1-4316-4c61-e906-fea5d0c7f3af"
      },
      "id": "6kzGPsvd6ZiB",
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.545373665480427"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we will write a function to get the predictions from our model:\n",
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      texts = d[\"text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"label\"].to(device)\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(outputs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return texts, predictions, prediction_probs, real_values"
      ],
      "metadata": {
        "id": "Crg1XFll6ba4"
      },
      "id": "Crg1XFll6ba4",
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  Roberta_model,\n",
        "  dev_DataLoader\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMjrHFuK6wkR",
        "outputId": "bce3ea04-2f6d-4aeb-bc4d-9e2ac871e672"
      },
      "id": "JMjrHFuK6wkR",
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "\n",
        "class_report=classification_report(y_test, y_pred, target_names=class_names)\n",
        "print(class_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LhSEtOI65pz",
        "outputId": "5c0b11ed-54f5-45a4-a328-0381657fd0ba"
      },
      "id": "1LhSEtOI65pz",
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "      moderate       0.59      0.63      0.61      2306\n",
            "not depression       0.59      0.44      0.50      1830\n",
            "        severe       0.28      0.52      0.37       360\n",
            "\n",
            "      accuracy                           0.55      4496\n",
            "     macro avg       0.49      0.53      0.49      4496\n",
            "  weighted avg       0.57      0.55      0.55      4496\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(),rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(),rotation=30, ha='right')\n",
        "  plt.ylabel('True Label')\n",
        "  plt.xlabel('Predicted Label');\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "1gAUBJ0567kH",
        "outputId": "08c8de53-f450-4961-ea29-ce5be544be11"
      },
      "id": "1gAUBJ0567kH",
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarize the targets & predictions to plot PR & ROC curves\n",
        "y_test= label_binarize(y_test, classes=[*range(n_classes)])\n",
        "y_pred = label_binarize(y_pred, classes=[*range(n_classes)])"
      ],
      "metadata": {
        "id": "jQqd7wKs7D8a"
      },
      "id": "jQqd7wKs7D8a",
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot PR (Precesion-Recall) curve:\n",
        "# plot model precision-recall curve\n",
        "plt.figure(figsize=(12,6))\n",
        "precision = dict()\n",
        "recall = dict()\n",
        "for i in range(n_classes):\n",
        "    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n",
        "                                                        y_pred[:, i])\n",
        "    plt.plot(recall[i], precision[i], lw=5, label='class {}'.format(i))\n",
        "\n",
        "plt.xlabel(\"recall\",fontsize=22)\n",
        "plt.ylabel(\"precision\",fontsize=22)\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"precision vs. recall curve\",fontsize=22)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "a-uymH8B7GVi",
        "outputId": "4b00cbfb-14fa-4db4-8c34-d910450c76e3"
      },
      "id": "a-uymH8B7GVi",
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ROC (Receiver operating characterstic) curve:\n",
        "# plot roc curve\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i],\n",
        "                                  y_pred[:, i])\n",
        "    plt.plot(fpr[i], tpr[i], lw=5, label='class {}'.format(i))\n",
        "\n",
        "plt.xlabel(\"false positive rate\",fontsize=22)\n",
        "plt.ylabel(\"true positive rate\",fontsize=22)\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"ROC curve\",fontsize=22)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "aibUqXXw7MaF",
        "outputId": "b7c5ea0c-0e8c-44de-8646-cdd53d380085"
      },
      "id": "aibUqXXw7MaF",
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CxW0dHIk7RS8"
      },
      "id": "CxW0dHIk7RS8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}